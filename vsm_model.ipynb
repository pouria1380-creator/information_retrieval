{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlOGy+HhEAl4/TCj7e2lzQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pouria1380-creator/information_retrieval/blob/main/vsm_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKhT2swGumwT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a95cc0a"
      },
      "source": [
        "# Task\n",
        "Create a VSM (Vector Space Model) for information retrieval with an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62c68585"
      },
      "source": [
        "## Define the corpus\n",
        "\n",
        "### Subtask:\n",
        "Create a small collection of documents to work with.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a1c76d6"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a list of strings to represent the documents in the corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6d3d5f7"
      },
      "source": [
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The lazy dog is sleeping on the mat.\",\n",
        "    \"The quick brown fox is a fast animal.\"\n",
        "]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e789adb"
      },
      "source": [
        "## Tokenize the documents\n",
        "\n",
        "### Subtask:\n",
        "Break down each document into individual words or tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ff9dc9"
      },
      "source": [
        "**Reasoning**:\n",
        "Tokenize each document in the corpus by converting to lowercase and splitting by whitespace.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03c07295"
      },
      "source": [
        "tokenized_corpus = []\n",
        "for doc in corpus:\n",
        "  tokenized_corpus.append(doc.lower().split())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d70265c"
      },
      "source": [
        "## Create a vocabulary\n",
        "\n",
        "### Subtask:\n",
        "Generate a unique list of all the tokens from the corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "834dee33"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate a unique list of all the tokens from the corpus by iterating through the tokenized documents and adding each token to a set, then converting the set to a list and printing it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5869c54",
        "outputId": "1b1220c8-8e94-4cfc-a527-2d0b2f477a8f"
      },
      "source": [
        "vocabulary = set()\n",
        "for doc in tokenized_corpus:\n",
        "  for token in doc:\n",
        "    vocabulary.add(token)\n",
        "\n",
        "vocabulary = list(vocabulary)\n",
        "print(vocabulary)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'jumps', 'is', 'brown', 'over', 'a', 'fast', 'lazy', 'dog.', 'quick', 'sleeping', 'fox', 'on', 'animal.', 'mat.', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54889b69"
      },
      "source": [
        "## Build the term-document matrix\n",
        "\n",
        "### Subtask:\n",
        "Represent each document as a vector based on the frequency of each term in the vocabulary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43c1aeb2"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the steps to create the term-document matrix based on term frequency.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3ac4c6d",
        "outputId": "927f925a-38d6-4a62-90a6-d3a6342e1929"
      },
      "source": [
        "term_document_matrix = []\n",
        "for doc in tokenized_corpus:\n",
        "  frequency_vector = [0] * len(vocabulary)\n",
        "  for token in doc:\n",
        "    try:\n",
        "      token_index = vocabulary.index(token)\n",
        "      frequency_vector[token_index] += 1\n",
        "    except ValueError:\n",
        "      # Handle tokens not in vocabulary if necessary, though in this case\n",
        "      # the vocabulary is built from the corpus itself.\n",
        "      pass\n",
        "  term_document_matrix.append(frequency_vector)\n",
        "\n",
        "# Display the term-document matrix\n",
        "for row in term_document_matrix:\n",
        "    print(row)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0]\n",
            "[2, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1]\n",
            "[1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d3f45fb"
      },
      "source": [
        "## Calculate tf-idf weights\n",
        "\n",
        "### Subtask:\n",
        "Apply TF-IDF (Term Frequency-Inverse Document Frequency) weighting to the term-document matrix to give more importance to terms that are unique to a document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da48e223"
      },
      "source": [
        "**Reasoning**:\n",
        "Apply TF-IDF weighting to the term-document matrix to give more importance to terms that are unique to a document.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a901ace6",
        "outputId": "26872453-0119-463d-d3b5-be501473a1dd"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Instantiate a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the TfidfVectorizer to the original corpus and transform it\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert the resulting TF-IDF matrix to a dense NumPy array\n",
        "tfidf_matrix = tfidf_matrix.todense()\n",
        "\n",
        "# Store the TF-IDF matrix in a variable (already done in the previous step)\n",
        "\n",
        "# Display the TF-IDF matrix\n",
        "print(tfidf_matrix)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.30330642 0.30330642 0.         0.30330642 0.\n",
            "  0.3988115  0.30330642 0.         0.         0.3988115  0.30330642\n",
            "  0.         0.47108899]\n",
            " [0.         0.         0.30716074 0.         0.         0.30716074\n",
            "  0.         0.30716074 0.40387946 0.40387946 0.         0.\n",
            "  0.40387946 0.47707544]\n",
            " [0.46312056 0.35221512 0.         0.46312056 0.35221512 0.35221512\n",
            "  0.         0.         0.         0.         0.         0.35221512\n",
            "  0.         0.27352646]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7e9dfc5"
      },
      "source": [
        "## Query processing\n",
        "\n",
        "### Subtask:\n",
        "Tokenize a sample query and represent it as a vector using the same vocabulary and weighting scheme.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38297e92"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a sample query, tokenize it, and transform it into a TF-IDF vector using the previously fitted vectorizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62da0a1f",
        "outputId": "e465044d-1d49-48ca-e423-778a732e67e9"
      },
      "source": [
        "sample_query = \"quick fox\"\n",
        "\n",
        "# Tokenize the query\n",
        "tokenized_query = sample_query.lower().split()\n",
        "print(\"Tokenized Query:\", tokenized_query)\n",
        "\n",
        "# Transform the tokenized query into a TF-IDF vector using the fitted vectorizer\n",
        "# The vectorizer expects a list of strings, so wrap the query in a list\n",
        "query_tfidf_vector = vectorizer.transform([sample_query])\n",
        "\n",
        "# Convert the resulting TF-IDF vector to a dense NumPy array\n",
        "query_tfidf_vector = query_tfidf_vector.todense()\n",
        "\n",
        "# Display the TF-IDF vector for the query\n",
        "print(\"Query TF-IDF Vector:\")\n",
        "print(query_tfidf_vector)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Query: ['quick', 'fox']\n",
            "Query TF-IDF Vector:\n",
            "[[0.         0.         0.         0.         0.70710678 0.\n",
            "  0.         0.         0.         0.         0.         0.70710678\n",
            "  0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f36d90"
      },
      "source": [
        "## Calculate similarity\n",
        "\n",
        "### Subtask:\n",
        "Compute the similarity between the query vector and each document vector using cosine similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4adf10fc"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to compute the cosine similarity between the query vector and each document vector. This requires importing the `cosine_similarity` function and applying it to the previously generated TF-IDF vectors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25264498",
        "outputId": "a3150f74-32d0-45fa-8f91-1a6db82fe552"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Convert the matrices to numpy arrays\n",
        "query_tfidf_vector_array = np.asarray(query_tfidf_vector)\n",
        "tfidf_matrix_array = np.asarray(tfidf_matrix)\n",
        "\n",
        "# Calculate the cosine similarity between the query vector and each document vector\n",
        "similarity_scores = cosine_similarity(query_tfidf_vector_array, tfidf_matrix_array)\n",
        "\n",
        "# Print or display the similarity scores\n",
        "print(\"Similarity Scores:\")\n",
        "print(similarity_scores)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Scores:\n",
            "[[0.42894006 0.         0.4981074 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6846cc31",
        "outputId": "5887bdff-82b2-4391-b281-67c9c842d824"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert the matrices to numpy arrays\n",
        "query_tfidf_vector_array = np.asarray(query_tfidf_vector)\n",
        "tfidf_matrix_array = np.asarray(tfidf_matrix)\n",
        "\n",
        "# Calculate the cosine similarity between the query vector and each document vector\n",
        "similarity_scores = cosine_similarity(query_tfidf_vector_array, tfidf_matrix_array)\n",
        "\n",
        "# Print or display the similarity scores\n",
        "print(\"Similarity Scores:\")\n",
        "print(similarity_scores)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Scores:\n",
            "[[0.42894006 0.         0.4981074 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb691aab",
        "outputId": "e5343fb4-93cc-4284-b3c3-3174ab08ea1e"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert the matrices to numpy arrays\n",
        "query_tfidf_vector_array = np.asarray(query_tfidf_vector)\n",
        "tfidf_matrix_array = np.asarray(tfidf_matrix)\n",
        "\n",
        "# Calculate the cosine similarity between the query vector and each document vector\n",
        "similarity_scores = cosine_similarity(query_tfidf_vector_array, tfidf_matrix_array)\n",
        "\n",
        "# Print or display the similarity scores\n",
        "print(\"Similarity Scores:\")\n",
        "print(similarity_scores)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Scores:\n",
            "[[0.42894006 0.         0.4981074 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e09e791f"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that `cosine_similarity` does not support `np.matrix` as input. Both `query_tfidf_vector` and `tfidf_matrix` were converted to dense matrices using `.todense()`. They need to be converted to NumPy arrays using `np.asarray` before calculating the cosine similarity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0FezN9UvUSW",
        "outputId": "c73aac26-a5ac-4647-c8ed-dd80e39fa49f"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert the matrices to numpy arrays\n",
        "query_tfidf_vector_array = np.asarray(query_tfidf_vector)\n",
        "tfidf_matrix_array = np.asarray(tfidf_matrix)\n",
        "\n",
        "# Calculate the cosine similarity between the query vector and each document vector\n",
        "similarity_scores = cosine_similarity(query_tfidf_vector_array, tfidf_matrix_array)\n",
        "\n",
        "# Print or display the similarity scores\n",
        "print(\"Similarity Scores:\")\n",
        "print(similarity_scores)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Scores:\n",
            "[[0.42894006 0.         0.4981074 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ff5338"
      },
      "source": [
        "## Rank documents\n",
        "\n",
        "### Subtask:\n",
        "Rank the documents based on their similarity to the query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad2c8802"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to rank the documents based on their similarity scores. I will create a list of tuples with document index and score, sort them, and then print the ranked indices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d0c243b",
        "outputId": "4b672669-4182-4613-9058-bc54de5c9e8d"
      },
      "source": [
        "# Create a list of tuples with document index and similarity score\n",
        "document_scores = [(i, score) for i, score in enumerate(similarity_scores[0])]\n",
        "\n",
        "# Sort the list in descending order based on similarity scores\n",
        "document_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Extract the ranked document indices\n",
        "ranked_documents = [doc[0] for doc in document_scores]\n",
        "\n",
        "# Print the ranked document indices\n",
        "print(\"Ranked Documents (by index):\")\n",
        "print(ranked_documents)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked Documents (by index):\n",
            "[2, 0, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82b51d9b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The corpus consists of three documents.\n",
        "*   After tokenization and building a vocabulary, there are 14 unique terms in the corpus.\n",
        "*   The TF-IDF matrix is a 3x14 matrix, where each row represents a document and each column represents a unique term, containing the TF-IDF scores.\n",
        "*   The sample query \"quick fox\" was successfully transformed into a TF-IDF vector using the same vectorizer fitted on the corpus.\n",
        "*   The cosine similarity scores between the query vector and the document vectors are `[[0.42894006 0.         0.4981074 ]]`, indicating the similarity to documents at index 0, 1, and 2 respectively.\n",
        "*   Based on the similarity scores, the documents are ranked in descending order of relevance to the query as follows: Document at index 2 (score 0.4981074), Document at index 0 (score 0.42894006), and Document at index 1 (score 0.0).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The document at index 2 (\"The quick brown fox is a fast animal.\") is the most relevant to the query \"quick fox\", followed by the document at index 0 (\"The quick brown fox jumps over the lazy dog.\"). The document at index 1 (\"The lazy dog is sleeping on the mat.\") is the least relevant, with a similarity score of 0.\n",
        "*   To improve the VSM, consider adding techniques like stemming or lemmatization to reduce terms to their root form and remove stop words to filter out common, less informative words.\n"
      ]
    }
  ]
}